{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation\n",
    "Untuk tugas akhir mata kuliah NLP Semester Genap 2018/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-1\n",
    "\n",
    "Spesifikasi model yang dibangun pada _notebook_ ini adalah sebagai berikut :\n",
    "\n",
    "- Data _trainset_ yang digunakan adalah `triple_annotator_agree.csv` dan `single_annotator.csv`.\n",
    "- _Preprocessing_ yang dilakukan pada kalimat adalah normalisasi kalimat dan _remove stopwords_.\n",
    "- _Feature_ yang digunakan adalah _Bag-Of-Words_.\n",
    "- Algoritma model yang digunakan adalah _Multinomial Naive Bayes_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import Sastrawi\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_SEED = 42\n",
    "np.random.seed(CUSTOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Read csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['double_annotator_agree.csv',\n",
       " 'double_annotator_disagree.csv',\n",
       " 'single_annotator.csv',\n",
       " 'triple_annotator_agree.csv',\n",
       " 'triple_annotator_disagree.csv']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path = '../dataset/training_set/'\n",
    "filenames = os.listdir(path)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kalimat_id</th>\n",
       "      <th>kata</th>\n",
       "      <th>sense</th>\n",
       "      <th>kalimat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000034</td>\n",
       "      <td>mengandung</td>\n",
       "      <td>2301</td>\n",
       "      <td>Di Jepang, manga biasanya serial di majalah ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000129</td>\n",
       "      <td>mengandung</td>\n",
       "      <td>2301</td>\n",
       "      <td>Surah ini dinamai Maryam, karena surat ini men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000476</td>\n",
       "      <td>mengandung</td>\n",
       "      <td>2301</td>\n",
       "      <td>Rebusan seperti Waterzooi atau Hachee, rebusan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000486</td>\n",
       "      <td>mengandung</td>\n",
       "      <td>2301</td>\n",
       "      <td>Permukaan daun mengandung lapisan lilin sehing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000511</td>\n",
       "      <td>mengandung</td>\n",
       "      <td>2301</td>\n",
       "      <td>Pemicu yang paling umum antara lain alergen, r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kalimat_id        kata sense  \\\n",
       "0     1000034  mengandung  2301   \n",
       "1     1000129  mengandung  2301   \n",
       "2     1000476  mengandung  2301   \n",
       "3     1000486  mengandung  2301   \n",
       "4     1000511  mengandung  2301   \n",
       "\n",
       "                                             kalimat  \n",
       "0  Di Jepang, manga biasanya serial di majalah ma...  \n",
       "1  Surah ini dinamai Maryam, karena surat ini men...  \n",
       "2  Rebusan seperti Waterzooi atau Hachee, rebusan...  \n",
       "3  Permukaan daun mengandung lapisan lilin sehing...  \n",
       "4  Pemicu yang paling umum antara lain alergen, r...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "for filename in filenames:\n",
    "    df = pd.read_csv(path + filename)\n",
    "    dfs.append(df)\n",
    "dfs[3].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Choose DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(783, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1439, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select dataframe from triple_annotator_agree.csv and single_annotator.csv\n",
    "annotator_agree = pd.DataFrame()\n",
    "\n",
    "for i,df in enumerate(dfs):\n",
    "    if i == 0 or i == 3:\n",
    "        annotator_agree = pd.concat([annotator_agree, df], ignore_index=True)\n",
    "\n",
    "annotator_agree.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotator_agree.kata.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kalimat_id</th>\n",
       "      <th>kata</th>\n",
       "      <th>sense</th>\n",
       "      <th>kalimat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000527</td>\n",
       "      <td>mengandung</td>\n",
       "      <td>2301</td>\n",
       "      <td>Secara anatomi, hidung adalah penonjolan pada ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000651</td>\n",
       "      <td>mengandung</td>\n",
       "      <td>2301</td>\n",
       "      <td>Mandi air yang mengandung belerang, untuk peng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000770</td>\n",
       "      <td>mengandung</td>\n",
       "      <td>2301</td>\n",
       "      <td>Dengan terikatnya klathrin, membrane sel membe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>994570</td>\n",
       "      <td>mengandung</td>\n",
       "      <td>2301</td>\n",
       "      <td>Karena mengandung PABA (Para Aminobenzoic Acid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>994574</td>\n",
       "      <td>mengandung</td>\n",
       "      <td>2301</td>\n",
       "      <td>Kopi robusta dapat dikatakan sebagai kopi kela...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     kalimat_id        kata sense  \\\n",
       "0       1000527  mengandung  2301   \n",
       "1       1000651  mengandung  2301   \n",
       "2       1000770  mengandung  2301   \n",
       "646      994570  mengandung  2301   \n",
       "647      994574  mengandung  2301   \n",
       "\n",
       "                                               kalimat  \n",
       "0    Secara anatomi, hidung adalah penonjolan pada ...  \n",
       "1    Mandi air yang mengandung belerang, untuk peng...  \n",
       "2    Dengan terikatnya klathrin, membrane sel membe...  \n",
       "646  Karena mengandung PABA (Para Aminobenzoic Acid...  \n",
       "647  Kopi robusta dapat dikatakan sebagai kopi kela...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = annotator_agree[annotator_agree.kata == 'mengandung']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets preprocessing for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stopwords = pd.read_csv('../dataset/stopwords.csv')\n",
    "    special_list = []\n",
    "    token = nltk.word_tokenize(text)\n",
    "    token_afterremoval = []\n",
    "    for k in token:\n",
    "        if k not in stopwords and k not in special_list:\n",
    "            token_afterremoval.append(k)\n",
    "    \n",
    "    str_clean = ' '.join(token_afterremoval)\n",
    "    return str_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normal_txt = text.lower()\n",
    "    normal_txt = re.sub('\\s+', ' ', normal_txt)\n",
    "    normal_txt = normal_txt.strip()\n",
    "    normal_txt = re.sub(r'[^\\w\\s]', '', normal_txt)\n",
    "    \n",
    "    normal_regex = re.compile(r'(.)\\1{1,}', re.IGNORECASE)\n",
    "    normal_txt = normal_regex.sub(r'\\1\\1', normal_txt)\n",
    "    return normal_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(texts):\n",
    "    text_clean = []\n",
    "    for txt in texts:\n",
    "        normal_txt = normalize(txt)\n",
    "        nosw_txt = remove_stopwords(normal_txt)\n",
    "        text_clean.append(nosw_txt)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['secara anatomi hidung adalah penonjolan pada vertebrata yang mengandung nostril yang menyaring udara untuk pernapasan',\n",
       " 'mandi air yang mengandung belerang untuk pengobatan penyakit kulit',\n",
       " 'dengan terikatnya klathrin membrane sel membentuk vesikel yang mengandung molekul ligan']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get clean texts\n",
    "raw_text = annotator_agree['kalimat']\n",
    "label = annotator_agree['sense'].tolist()\n",
    "\n",
    "clean_text = preprocessing(raw_text)\n",
    "clean_text[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Word ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_id(sense_id):\n",
    "    assert len(sense_id) == 4\n",
    "    return sense_id[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['23' '23' '23' '28' '28' '28' '28' '28' '28' '28']\n"
     ]
    }
   ],
   "source": [
    "# Get wid\n",
    "sense_id = annotator_agree.sense\n",
    "wid = np.array(list(map(extract_word_id, sense_id)))\n",
    "print(wid[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bag-Of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "['006', '010', '04', '075', '0800', '08kepyappti1986', '10', '100', '100500', '1020']\n"
     ]
    }
   ],
   "source": [
    "def extract_bag_of_words(text):\n",
    "    unigram = CountVectorizer(ngram_range=(1,1))\n",
    "    unigram_matrix = unigram.fit_transform(np.array(text)).todense()\n",
    "    feat_name = unigram.get_feature_names()\n",
    "    return unigram_matrix, feat_name\n",
    "\n",
    "unigram_feat, feat_name = extract_bag_of_words(clean_text)\n",
    "print(unigram_feat[:3])\n",
    "print(feat_name[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-99c9dcc5d767>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m classifier = Pipeline([\n\u001b[0;32m      2\u001b[0m     ('features', FeatureUnion([\n\u001b[1;32m----> 3\u001b[1;33m         ('words', Pipeline([\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         ])),\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, steps, memory)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_validate_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;31m# validate names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "classifier = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('words', Pipeline([\n",
    "            \n",
    "        ])),\n",
    "        ('bow', Pipeline([\n",
    "            \n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', GaussianNB()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
